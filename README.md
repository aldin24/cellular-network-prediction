ğŸ“¡ Predicting Cellular Network Signal Instability Using Machine Learning & Deep Learning

This repository contains the full implementation of a research study comparing traditional machine learning models and deep learning sequence models for predicting LTE downlink throughput stability using a publicly available Kaggle dataset.

The goal is to evaluate whether deep learning (LSTM, GRU, CNN, CNNâ€“LSTM) can outperform classical ML methods (Linear Regression, Decision Trees, Random Forests, XGBoost) when applied to small real-world cellular measurement datasets.

ğŸ“ Repository Structure
â”œâ”€â”€ data_loader.py
â”œâ”€â”€ preprocess_shared.py
â”œâ”€â”€ preprocess_deep.py
â”œâ”€â”€ regression_models/
â”‚   â”œâ”€â”€ linear_regression.py
â”‚   â”œâ”€â”€ decision_tree.py
â”‚   â”œâ”€â”€ random_forest.py
â”‚   â”œâ”€â”€ xgboost_model.py
â”œâ”€â”€ deep_models/
â”‚   â”œâ”€â”€ cnn.py
â”‚   â”œâ”€â”€ gru.py
â”‚   â”œâ”€â”€ lstm.py
â”‚   â”œâ”€â”€ cnn_lstm.py
â”œâ”€â”€ main_regression.py
â”œâ”€â”€ main_deep.py
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

ğŸš€ 1. Overview

Cellular networks generate large volumes of radio measurements such as RSRP, RSRQ, RSSI, CQI, SNR, and throughput. Predicting downlink throughput instability helps operators:

optimize radio resources

detect bad coverage areas

improve Quality of Service (QoS)

support self-optimizing networks

This project evaluates multiple ML & DL models to determine which architecture best predicts LTE downlink Mbps.

ğŸ”§ 2. Installation
1. Clone the repository
git clone https://github.com/<your-username>/cellular-network-prediction.git
cd cellular-network-prediction

2. Create virtual environment
python3 -m venv .venv
source .venv/bin/activate

3. Install dependencies
pip install -r requirements.txt

4. Install TensorFlow for macOS (Apple Silicon)
pip install tensorflow-macos tensorflow-metal

ğŸ“¦ 3. Dataset

The project uses the public Kaggle dataset:

ğŸ“Š aeryss/lte-dataset
(LTE cellular drive-test measurements)

Downloaded automatically through:

from data_loader import load_kaggle_dataset

ğŸ§¹ 4. Preprocessing Steps
âœ” Clean raw LTE measurements

Remove invalid RSRQ values

Drop unused columns

Filter only connected-state samples (State = â€œDâ€)

Handle numeric anomalies

âœ” Feature Engineering

Distance bands

Speed categories

Lag features (for DL models)

Cyclical time features (Hour/Day sinâ€“cos)

âœ” Scaling

ML models â†’ StandardScaler

DL models â†’ MinMaxScaler

âœ” Sequence creation (Deep Learning only)

Group by CellID

Create sliding windows of size N

Preserve time order

ğŸ¤– 5. Models Implemented
Machine Learning Models
Model	File	Notes
Linear Regression	linear_regression.py	Baseline
Polynomial Regression	integrated	Adds nonlinearity
Decision Tree	decision_tree.py	Captures nonlinear thresholds
Random Forest	random_forest.py	Reduces overfitting
XGBoost	xgboost_model.py	â­ Best performer
Deep Learning Models
Model	File	Notes
LSTM	lstm.py	Temporal modeling
GRU	gru.py	Lighter version of LSTM
CNN	cnn.py	Extracts spatial features
CNNâ€“LSTM hybrid	cnn_lstm.py	Spatial + Temporal
ğŸ§  6. Running Experiments
Run classical ML experiments
python main_regression.py

Run deep learning experiments
python main_deep.py


Each file prints:

MAE

RMSE

RÂ²

Graph showing prediction vs actual throughput
